# -*- coding: utf-8 -*-
"""Diabetes_Onset_logistic_regression_Brent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k5bGP6hVqTALqccLl47LP-mddnYt4vOY

# Pima Indians Onset of Diabetes - Logistic Regression

***This is for classification where we predict a category or class (a binary variable: 0, 1).***

**Dataset Used:** Pima Indians onset of diabetes dataset (CSV).

**Data Source:** UCI Machine Learning repository. 

**Data Overview**: N = 758 patients

It describes patient medical record data for female (at least 21 years of age) Pima Indians and whether they had an onset of diabetes within five years.

**Problem Type:** binary classification problem (onset of diabetes as 1 or not as 0)

**Input Variables (X):**

1) Number of times pregnant
2) Plasma glucose concentration a 2 hours in an oral glucose tolerance test
3) Diastolic blood pressure (mm Hg)
4) Triceps skin fold thickness (mm)
5) 2-Hour serum insulin (mu U/ml)
6) Body mass index (weight in kg/(height in m)^2)
7) Diabetes pedigree function
8) Age (years)

**Output Variable (y):**

Class variable (0 or 1)

# Importing the libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

# Plots
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import plotly.offline as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import plotly.tools as tls
import plotly.figure_factory as ff
py.init_notebook_mode(connected=True)

"""# Importing the dataset"""

dataset = pd.read_csv('pima-indians-diabetes.csv') #dataset has no column headers so header = None
X = dataset.iloc[:, :-1].values #X inputs (features) are all rows and all columns EXCEPT the last one.
y = dataset.iloc[:, -1].values #y output (dependent variable) includes all rows but ONLY the last column.

"""The features of the dataset need to be in the 1st columns and the depedent variable needs to be in the last (right-most) column. 

**Line 2**: X = dataset.iloc[:, :-1].values

Selects all the columns and their rows,  except the last column.

[:, :-1] <--  the colon (:) means all of the rows are to be selected. The :-1 means all of the columns except the last one are selected. So [:, :-1] means select all of the columns up to, but not including, the last one and all of the columns' rows. 

**Line 3**: y = dataset.iloc[:, -1].values

Selects the last column only and all of its rows, regardless of the number of features in the dataset. [ :, -1 ] where the : means all the rows and the - 1 means all the columns except the last one....so all the rows for the last column only.

"""

#Checking the dataset's info and head (1st 5 entries)
display(dataset.info(),dataset.head())

#summarize the data
print(dataset.describe())

"""# Cleaning The Data

If you look through the min. (minimum) data in the 'summarize the data' section above, you'll notice a lot of zeros (0's). Here are the columns for which values of zero don't make sense (the patient would be deceased with a zero in any of these columns):

*   Glucose
*   Blood Pressure
*   SkinThickness
*   Insulin
*   BMI

Let's look at how many 0 values there are by column (excluding that last column (outpout) which SHOULD have 1's and 0's).






"""

#Finding Zeros
def finding_zeros(frame):
  columns = frame.columns[:8]  #The column labels of the DataFrame...the first 8 columns.
  for i in columns:
    zeros = len(frame.loc[frame[i] == 0])  #This line counts the number of 0 values in each column. frame.loc >> Access a group of rows and columns by label(s) or a boolean array..loc[] is primarily label based, but may also be used with a boolean array.
    print(f'The number of 0 values in {i} = {zeros}') #This is an f-string: The idea behind f-strings is to make string interpolation simpler. To create an f-string, prefix the string with the letter “ f ”.  

finding_zeros(dataset)

"""As you can see above, there are a LOT of zero values. I'll replace the zeros with NaN."""

dataset[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = dataset[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0,np.NaN)

"""Let's plot the missing data points now."""

# Boxplots
plt.style.use('ggplot') # Using ggplot2 style visuals 
f, ax = plt.subplots(figsize=(11, 15))
ax.set_facecolor('#fafafa')
ax.set(xlim=(-0.05,400))
plt.ylabel('Variables')
plt.title('Overview Data Set')
ax = sns.boxplot(data = dataset, orient = 'h', palette = 'Set2')

"""In the boxplots above, you can see the zero values are encoded with NaN values now.

# Splitting the dataset into the Training set and Test set
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

"""***Lines 1 & 2 will create a test set and training set from the entire dataset we uploaded/imported.***

**Line 2**: X_train, X_test, y_train, y_test = train_test_split(**X**, **y**, test_size = 0.20, random_state = 0)

Where **X** is the matrix of features X and **y** is the the dependent variable vector y. 

***test_size *** 20% of data for testing is standard. That leaves 80% for training.
"""

print(X_train)

print(y_train)

print(X_test) #<---X_test is supposed to be new data in production. We'll pretend X_test is new observations we can apply the model to to see how it predicts.

print(y_test) #<--prints all predictions (0 or 1) for the test set.

"""# Feature Scaling

**Why feature scaling here? It's not required for logistic regression, but applying it will improve the training performance and the final predictions.** 

**For some models liks SVR, feature scaling is an absolute necessity.**
"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""**Lines 1 -4**: 

We want to feature scale our X inputs ('features'). We do NOT need to feature scale the depedent variable (diabetes onset or no onset) since it's already 0, 1 for values. As shown in the print-out below, the two features now have values from -3 to +3 which is perfect.

**>>> Feature scaling MUST happen AFTER we split the training set and test set so we avoid information leakage from the test set.**  


"""

print(X_train)

print(X_test)

"""# Training the Logistic Regression model on the Training set"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

"""**Line 2**: classifier = LogisticRegression(random_state = 0)

***random_state = 0*** <---Normally would not have to enter this parameter....so in the future, omit it.

Line 2 simply builds the Logistic Regression model.  

**Line 3**: classifier.fit(X_train, y_train)

Trains classifier on the training set.You need to call the fit method which takes two inputs (sets of data): the matrix of features of the training set and the dependent variable vector (the last column of data in the dataset).

# Predicting a new result
"""

print(classifier.predict(sc.transform([[1, 199, 76, 43, 0.0, 42.9, 1.394, 22]]))) #<--predicts 0 or 1 (no or yes)

print(classifier.predict_proba(sc.transform([[1, 199, 76, 43, 0.0, 42.9, 1.394, 22]]))) #<--predicts a PROBABILITY

"""***The transform method is used here so it matches the same format of the data in the training set.***

***sc = StandardScaler***

**print(classifier.predict(sc.transform([[1, 199, 76, 43, 0.0, 42.9, 1.394, 22]])))**

Enter the data for the 1st patient in the X_test set. We'll enter these inputs and predict whether or not she has an onset of diabetes. This is the 1st person in X_test dataset above. The model predicted this person has an onset of diabetes which is a TRUE (correct prediction. 

**print(classifier.predict_proba(sc.transform([[1, 199, 76, 43, 0.0, 42.9, 1.394, 22]])))**:  We can use this instead to produce a probability of whether or not the female patient has an onset of diabetes. For the 1st patient in the dataset, the result it: [[0.03968903 0.96031097]] meaning there's a 4% chance she does not and a 96% chance she does have an onset.

# Predicting the Test set results
"""

y_pred = classifier.predict(X_test) 
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) #Column 1 = predicted result, column 2 = actual results

"""***Column 1 = predicted result, column 2 = actual results***

# Making the Confusion Matrix For The TEST Set
"""

from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score
from sklearn.metrics import ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
print(cm)

print('Accuracy Score:',accuracy_score(y_test, y_pred))
print('Recall Score:', recall_score(y_test, y_pred))     
print('Precision Score:', precision_score(y_test, y_pred))

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)
disp.plot()

"""**Line 2**: cm = confusion_matrix(y_test, y_pred)

Creates a confusion matrix of the y_test variables vs. the y_pred (predicted) outcomes. 

**Confusion Matrix Interpretation**:  This is for the TEST set only.

[[**98**   **9**] <--**98** correct predictions of the class zero (0; no onset of diabetes), **9** incorrect predictions of class 1 (false positives) meaning 9 predictions that patients HAD onset of diabetes but in reality they did not.

 [ **18   29**]] <--**18** type II errors (false negatives)...these are very bad errors, **29** correc predictions for patients with an onset of diabetes (who really had it).

 **Lines 6-8**: Various metrics for the model.

**Line 6**: accuracy_score(y_test, y_pred)

Total correct predictions / total observations; accuracy_score = (98+29)/154 total observations 

**Line 7**: Provides the accuracy score on the test set.

 **Recall_Score**: Recall Score: 0.6170212765957447 (~62%)

 Formula: TP/(TP + FN); 29/(29+18) = 0.617

The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.
 
This is a really important score as it tells how good the model is at minimizing false negatives (type II errors...the really bad erros to make). You want to avoid predicting that a patient does NOT have a medical condition when they really do have a medical condition. They may not go on to receive treatment and that is a very bad.

**Dimensions**: Each feature (input X) correpsonds to a dimension. In this example, we have 8 features (8 dimensions) so we cannot visualize the plot.Humans cannot vizualize more than 3 dimensions (3D).

#Conclusion

Given the recall score is 61%, this isn't very good. With medical applications, Type II errors (False-Negative) can be very dangerous as patients have a medical condition but the model is predicting they do not. As a result, the patient may not go on to receive the proper diagnosis and treatment. 

**How might I improve this? **

I noticed quite a few columns have zero values which don't make sense. For example, a patient cannot have a diastolic blood pressure of zero, otherwise they would be deceased. I believe by dealing with nonsensical zero values, the model would have an improved overall accuracy score along with a recall score.
"""